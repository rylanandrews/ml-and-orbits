# -*- coding: utf-8 -*-
"""ml_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mxvqJNEKPx3tXvWJrz67ThqS7pqcOJEJ
"""

# Imports for TensorFlow, Keras, and NumPy
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow import keras
import numpy as np

#tf.nn.leaky_relu

# Create the model structure
model = Sequential([
      Dense(256, activation='swish', input_shape=[21]),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(256, activation='swish'),
      Dense(8),
])

model.summary()

# Mount drive in order to import training/validation data
from google.colab import drive
drive.mount('/content/drive')

# Used for importing data and measuring performance
import csv
import time

# Import the training data

trainPath = 'drive/My Drive/scienceResearch2021Data/training/'

# Recreate the time points for the simulation
time_span = np.linspace(0, 3, 1500)
times = np.array(time_span)
times = times[:,None]

# Dataset variables to be populated with data from csv files
trainingInputs = []
trainingOutputs = []

# Track performance of program
s = time.perf_counter()
counter = 0

#print("Creating trainingInputs array...")
# Use master csv file to create the training input array
#with open(trainPath + 'master.csv', newline='') as masterFile:
#  masterReader = csv.reader(masterFile, delimiter=',')
#  # Loop through each row in master file
#  for masterRow in masterReader:
#    # Since there are 1500 data points per solution, create an array with 1500 rows of initial conditions
#    initConditions = np.full((1500, 20), np.asarray(masterRow[2:]).astype('float32'))
#    # Add the times to array
#    inputs = np.concatenate((times, initConditions), axis=1)
#    # Append resulting array to training inputs array
#    if (len(trainingInputs) == 0):
#      trainingInputs = inputs
#    else:
#      trainingInputs = np.append(trainingInputs, inputs, axis=0)
#    counter += 1
#    print(str(counter))

print("Creating trainingInputs array...")
trainingInputs = np.load(trainPath + "master.npy")

print("Creating trainingOutputs array...")
trainingOutputs = np.load(trainPath + "trainingOutputs.npy")

print(len(trainingInputs))
print(len(trainingOutputs))

e = time.perf_counter()
elapsed = e - s
print(str(elapsed) + " s")

trainingInputs = np.asarray(trainingInputs).astype('float32')
trainingOutputs = np.asarray(trainingOutputs).astype('float32')

# Import the validation data

valPath = 'drive/My Drive/scienceResearch2021Data/validation/'

# Recreate the time points for the simulation
time_span = np.linspace(0, 3, 1500)
times = np.array(time_span)
times = times[:,None]

# Dataset variables to be populated with data from csv files
validationInputs = []
validationOutputs = []

# Track performance of program
s = time.perf_counter()

print("Creating trainingInputs array...")
# Use master csv file to create the training input array
with open(valPath + 'master.csv', newline='') as masterFile:
  masterReader = csv.reader(masterFile, delimiter=',')
  # Loop through each row in master file
  for masterRow in masterReader:
    # Since there are 1500 data points per solution, create an array with 1500 rows of initial conditions
    initConditions = np.full((1500, 20), np.asarray(masterRow[2:]).astype('float32'))
    # Add the times to array
    inputs = np.concatenate((times, initConditions), axis=1)
    # Append resulting array to training inputs array
    if (len(validationInputs) == 0):
      validationInputs = inputs
    else:
      validationInputs = np.append(validationInputs, inputs, axis=0)

print("Creating validationOutputs array...")
validationOutputs = np.load(valPath + "trainingOutputs.npy")

e = time.perf_counter()
elapsed = e - s
print(str(elapsed) + " s")

validationInputs = np.asarray(validationInputs).astype('float32')
validationOutputs = np.asarray(validationOutputs).astype('float32')

print("Training inputs length: " + str(len(trainingInputs)))
print("Training outputs length: " + str(len(trainingOutputs)))
print("Validation inputs length: " + str(len(validationInputs)))
print("Validation outputs length: " + str(len(validationOutputs)))

len(trainingInputs[0])

len(trainingOutputs[0])

trainingInputs[0]

trainingOutputs[0]

#!pip install h5py pyyaml

model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])

checkpoint_path = "drive/My Drive/scienceResearch2021Data/swish_checkpoints/cp.ckpt"
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1, period=5)

model.fit(trainingInputs, trainingOutputs, validation_data=(validationInputs, validationOutputs), batch_size=15_000, epochs=300, callbacks=[cp_callback])

sample = [[0.034023, 1.644106, 1.511701, 0.527347, 0.665565, -0.12554, -0.32979, -0.9573, -1.08647, 0.0959, -2.41366, -0.79072, -1.608, -0.02961, -0.02446, -0.00781, 0.03624, -0.00266, 0.018682, 0.003824, 0.039719]]
sampleArr = np.array(sample, dtype=float)
model.predict(sampleArr)

path = 'drive/My Drive/Schoolwork/12th Grade/Science Research 2021/model/1/'
tf.saved_model.save(model, path)